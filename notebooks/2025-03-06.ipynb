{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "auto_gradについて学ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 2.0, dy/dx = 4.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad = True)\n",
    "y = x ** 2\n",
    "y.backward()\n",
    "\n",
    "print(f\"x = {x.item()}, dy/dx = {x.grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0, dz/dx = 6.0\n",
      "4.0, dy/dz = 48.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(3.0, requires_grad=True)\n",
    "y = torch.tensor(4.0, requires_grad=True)\n",
    "\n",
    "z = x**2 + y** 3\n",
    "\n",
    "z.backward()\n",
    "\n",
    "print(f\"{x.item()}, dz/dx = {x.grad.item()}\")\n",
    "print(f\"{y.item()}, dy/dz = {y.grad.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AddBackward0 object at 0x7f12080c3940>\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(z.grad_fn)\n",
    "print(x.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 35.0, y_detached: 35.0\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "y = x ** 2  # y = 5^2 = 25\n",
    "\n",
    "y_detached = y.detach()  # `detach()` で計算グラフを切り離す\n",
    "\n",
    "# ✅ y_detached を変更するとどうなる？\n",
    "y_detached += 10\n",
    "\n",
    "print(f\"y: {y}, y_detached: {y_detached}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".detatchでは元のデータを参照している。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y: 25.0, requires_grad: False\n"
     ]
    }
   ],
   "source": [
    "\n",
    "x = torch.tensor(5.0, requires_grad=True)\n",
    "\n",
    "with torch.no_grad():  # ✅ このブロック内は勾配を計算しない\n",
    "    y = x ** 2\n",
    "    print(f\"y: {y}, requires_grad: {y.requires_grad}\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimizerについて学ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, w = 2.200000047683716, Loss = 1.0\n",
      "epoch 2, w = 2.3600001335144043, Loss = 0.6399999260902405\n",
      "epoch 3, w = 2.4880001544952393, Loss = 0.40959984064102173\n",
      "epoch 4, w = 2.590400218963623, Loss = 0.2621438503265381\n",
      "epoch 5, w = 2.6723201274871826, Loss = 0.16777198016643524\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "loss_fn = lambda w: (w - 3) ** 2\n",
    "\n",
    "optimizer = optim.SGD([w], lr = 0.1)\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss = loss_fn(w)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f\"epoch {epoch+1}, w = {w.item()}, Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1, w = 2.0999999046325684, Loss = 1.0\n",
      "epoch 2, w = 2.197052478790283, Loss = 0.8100001811981201\n",
      "epoch 3, w = 2.2937304973602295, Loss = 0.6447247266769409\n",
      "epoch 4, w = 2.3909058570861816, Loss = 0.4988166093826294\n",
      "epoch 5, w = 2.48895263671875, Loss = 0.3709956705570221\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "optimzer = optim.Adam([w], lr = 0.1)\n",
    "\n",
    "for epoch in range(5):\n",
    "    loss = loss_fn(w)\n",
    "    loss.backward()\n",
    "    optimzer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    print(f\"epoch {epoch+1}, w = {w.item()}, Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before step: w = 2.0, grad = -2.0\n",
      "After step: w = 2.200000047683716\n",
      "------------------------------\n",
      "Before step: w = 2.200000047683716, grad = -1.5999999046325684\n",
      "After step: w = 2.3600001335144043\n",
      "------------------------------\n",
      "Before step: w = 2.3600001335144043, grad = -1.2799997329711914\n",
      "After step: w = 2.4880001544952393\n",
      "------------------------------\n",
      "Before step: w = 2.4880001544952393, grad = -1.0239996910095215\n",
      "After step: w = 2.590400218963623\n",
      "------------------------------\n",
      "Before step: w = 2.590400218963623, grad = -0.8191995620727539\n",
      "After step: w = 2.6723201274871826\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# ✅ 最適化手法（SGD）\n",
    "optimizer = optim.SGD([w], lr=0.1)\n",
    "\n",
    "# ✅ 5エポック分の学習\n",
    "for epoch in range(5):\n",
    "    loss = (w - 3) ** 2  # 損失計算\n",
    "    loss.backward()  # 勾配計算\n",
    "    print(f\"Before step: w = {w.item()}, grad = {w.grad.item()}\")  # w.grad をチェック！\n",
    "\n",
    "    optimizer.step()  # w の更新\n",
    "    optimizer.zero_grad()  # 勾配リセット（しないと累積される！）\n",
    "\n",
    "    print(f\"After step: w = {w.item()}\")  # 更新後の w を表示\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "誤差関数について学ぶ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss: 0.75\n"
     ]
    }
   ],
   "source": [
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "y_true = torch.tensor([3.0, 5.0, 7.0])\n",
    "y_pred = torch.tensor([2.5, 4.0, 6.0])\n",
    "\n",
    "loss = mse_loss(y_true, y_pred)\n",
    "print(f\"MSE Loss: {loss.item()}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropy Loss: 1.4170299768447876\n"
     ]
    }
   ],
   "source": [
    "ce_loss = nn.CrossEntropyLoss()  # 交差エントロピー\n",
    "\n",
    "y_true = torch.tensor([1])  # 正解ラベル（クラス1）\n",
    "y_pred = torch.tensor([[2.0, 1.0, 0.1]])  # 3クラスの予測（未softmax）\n",
    "\n",
    "loss = ce_loss(y_pred, y_true)  # 損失計算\n",
    "print(f\"CrossEntropy Loss: {loss.item()}\")  # 低いほど良い"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (venv)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
