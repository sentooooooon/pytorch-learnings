# pytorch-learnings

### 2025-02-20:

今日は仮想環境のセットアップをした。classifierのソースコードをいろいろ動かしてみた。データセットとデータローダーの違いが分かるようになった。transformerは下処理に必要なもので、大体ToTensor()で行う。


### 2025-02-21:

今日は簡単なニューラルネットワークを作った。iteratorの使い方を学んで、それによって、データセットのデータのサイズを知れるようになった。モデルの学習もやってみたが、以外に学習が遅かった、最初に49%ぐらいで、エポック数5だったのに、50%ぐらいまでしか上がらなかった。

分らなかったことは.item()の使い方、optimizer, lossfunctionなど。明日は、学習結果の可視化をおこなう。


### 2025-02-23

今日はNNモデルの復習をまず行った。model()を組み立ててからoptimizerを定義しないとうまくいかないことを知った。
それと情報可視化を行った。matplotlibではエポック数ごとの損失率と、正答率をグラフにして可視化した。
tensorboardも少し触った。tensorboardでは必要な情報を保存し書き込むと自動で解析してwebサイトで見れるようになる。非常に便利なのでたくさん使っていきたい。
logをくらうどで保存しようと思ったが、できなかったのでローカルでいいや。

### 2025-02-25

今日はCNNのモデルを作ってみた。畳み込みをすることで、チャンネル数を増やすことができ、それぞれのチャンネルは様々な特徴をおさえている。プーリング層を挟むと、サイズが半分になる。正答率がかなり改善された。エポック数１０回くらいで80%を超えた。様々な層の順番を変えることでいろいろ結果も変わるらしい。明日はforward関数をいくつかつくって、いろんな順番でやってみようかな。

### 2025-02-28

今日は評価関数を作った。評価関数は、訓練関数の中で動かし、各エポックごとにテストするのが良い。きょうはあまり進まなかったが継続が大事である。

### 2025-03-06

しばらくREADMEを更新するのを忘れてしまった。今日はpytorchの基礎をコードを実装しながら学んだ。誤差関数、optimzer、auto_gradなどの使い方やこれらがどのように相互にかかわりながら動いているかを理解した。また誤差関数のCrossentropyなどで使われるsoftmaxの存在意義を知った。softmaxを使うことで出力の比率では差を考慮した勾配の調節ができるようになる。具体的には、１と２の比率と１００と２００の比率は同じだが差は違う。softmaxを使うことでこの差を考慮した誤差を出すことができる。