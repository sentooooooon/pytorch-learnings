# pytorch-learnings

### 2025-02-20:

今日は仮想環境のセットアップをした。classifierのソースコードをいろいろ動かしてみた。データセットとデータローダーの違いが分かるようになった。transformerは下処理に必要なもので、大体ToTensor()で行う。


### 2025-02-21:

今日は簡単なニューラルネットワークを作った。iteratorの使い方を学んで、それによって、データセットのデータのサイズを知れるようになった。モデルの学習もやってみたが、以外に学習が遅かった、最初に49%ぐらいで、エポック数5だったのに、50%ぐらいまでしか上がらなかった。

分らなかったことは.item()の使い方、optimizer, lossfunctionなど。明日は、学習結果の可視化をおこなう。


### 2025-02-23

今日はNNモデルの復習をまず行った。model()を組み立ててからoptimizerを定義しないとうまくいかないことを知った。
それと情報可視化を行った。matplotlibではエポック数ごとの損失率と、正答率をグラフにして可視化した。
tensorboardも少し触った。tensorboardでは必要な情報を保存し書き込むと自動で解析してwebサイトで見れるようになる。非常に便利なのでたくさん使っていきたい。
logをくらうどで保存しようと思ったが、できなかったのでローカルでいいや。

### 2025-02-25

今日はCNNのモデルを作ってみた。畳み込みをすることで、チャンネル数を増やすことができ、それぞれのチャンネルは様々な特徴をおさえている。プーリング層を挟むと、サイズが半分になる。正答率がかなり改善された。エポック数１０回くらいで80%を超えた。様々な層の順番を変えることでいろいろ結果も変わるらしい。明日はforward関数をいくつかつくって、いろんな順番でやってみようかな。

### 2025-02-28

今日は評価関数を作った。評価関数は、訓練関数の中で動かし、各エポックごとにテストするのが良い。きょうはあまり進まなかったが継続が大事である。

### 2025-03-06

しばらくREADMEを更新するのを忘れてしまった。今日はpytorchの基礎をコードを実装しながら学んだ。誤差関数、optimzer、auto_gradなどの使い方やこれらがどのように相互にかかわりながら動いているかを理解した。また誤差関数のCrossentropyなどで使われるsoftmaxの存在意義を知った。softmaxを使うことで出力の比率では差を考慮した勾配の調節ができるようになる。具体的には、１と２の比率と１００と２００の比率は同じだが差は違う。softmaxを使うことでこの差を考慮した誤差を出すことができる。


### 2025-03-07

今日は回帰モデルを作った。回帰モデルとは連続な出力を出すものである。今までやった分類モデルは、連続ではなかった。回帰モデルにはMSEなどがよく使われる。また回帰モデルには線形回帰と非線形回帰がある。今回は線形回帰モデルを作った。線形回帰は「特徴と出力の関係が足し算の形」で表せる場合に使える。

また、CNNも勉強した。レイヤーの種類などを再確認した。また、畳み込みをすることでデータのサイズが変わる理由やどのように変わるかを自分で導出できるようになった。特徴量を抽出するとき、ストライド１、カーネルサイズ３、パディング１だったらサイズは変わらない。プーリング層は、カーネルサイズ２、ストライド２でサイズを半分に縮小する。
MNISTを使った分類モデルを作ったが精度が98%ほどでかなりよかった。


### 2025-03-10

ハイパーパラメータとデータの拡張を学んだ。learning rateを小さくするとよりおおきなエポック数を必要とする。またバッチ数でも、小さいとよりそのデータに対応したモデルになり小さすぎると、過学習になる恐れがある。
データ拡張には様々な種類がある。左右反転するモノや、ノイズを加えるもの、画像を傾けるものなどがある。しかし、これらの拡張を使うときには気を付ける必要がある。例えば、手書きの数字を読み取るモデルでは、左右反転はあまり効果がない。


### 2025-03-12

今回は転移学習を行った。resnet18を使いまずは全結合層のみ学習を行った。lossが1.8->1.7とあまり大きな改善は見られなかった。次にfine turing を行った。fine turingを行うと、lossが、0.9 -> 0.2までと大きく改善された。またテストしてみると、正答率が81.32%となった。転移学習によって、より手軽にモデルを作ることができることが分かった。一方で、転移学習を使った学習はいつも扱うような簡単なモデルと比べ、時間がかかることが分かった。それぞれ10分くらいかかった。